{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "*At the end of the lecture you should be to*:\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "\n",
    "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
    "\n",
    "__Solution:__ Use an autoencoder\n",
    "\n",
    "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
    "\n",
    "* __Information Retrieval__\n",
    "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
    "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
    "* __Dimensionality Reduction__\n",
    "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
    "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
    "\n",
    "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Architecture (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
    "\n",
    "The learning process gis described simply as minimizing a loss function: \n",
    "$ L(x, g(f(x))) $\n",
    "\n",
    "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
    "- $f$ is the encoder function\n",
    "- $g$ is the decoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "### Extremely Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Model is blue - red - blue layers (input - reduce - output), 784 to 32 \n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder 28x28 = 784\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim,activation='sigmoid')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "\n",
    "# create the decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.6951 - val_loss: 0.6948\n",
      "Epoch 2/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6946 - val_loss: 0.6943\n",
      "Epoch 3/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6942 - val_loss: 0.6939\n",
      "Epoch 4/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6938 - val_loss: 0.6935\n",
      "Epoch 5/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6934 - val_loss: 0.6931\n",
      "Epoch 6/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 7/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6925 - val_loss: 0.6923\n",
      "Epoch 8/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6921 - val_loss: 0.6918\n",
      "Epoch 9/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6917 - val_loss: 0.6914\n",
      "Epoch 10/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6913 - val_loss: 0.6910\n",
      "Epoch 11/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6909 - val_loss: 0.6906\n",
      "Epoch 12/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6904 - val_loss: 0.6902\n",
      "Epoch 13/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6900 - val_loss: 0.6898\n",
      "Epoch 14/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6896 - val_loss: 0.6893\n",
      "Epoch 15/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6892 - val_loss: 0.6889\n",
      "Epoch 16/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6888 - val_loss: 0.6885\n",
      "Epoch 17/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6884 - val_loss: 0.6881\n",
      "Epoch 18/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6880 - val_loss: 0.6877\n",
      "Epoch 19/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6876 - val_loss: 0.6873\n",
      "Epoch 20/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6871 - val_loss: 0.6869\n",
      "Epoch 21/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6867 - val_loss: 0.6865\n",
      "Epoch 22/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6863 - val_loss: 0.6860\n",
      "Epoch 23/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6859 - val_loss: 0.6856\n",
      "Epoch 24/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6855 - val_loss: 0.6852\n",
      "Epoch 25/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6851 - val_loss: 0.6848\n",
      "Epoch 26/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6847 - val_loss: 0.6844\n",
      "Epoch 27/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6843 - val_loss: 0.6840\n",
      "Epoch 28/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6838 - val_loss: 0.6836\n",
      "Epoch 29/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6834 - val_loss: 0.6832\n",
      "Epoch 30/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6830 - val_loss: 0.6828\n",
      "Epoch 31/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6826 - val_loss: 0.6824\n",
      "Epoch 32/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6822 - val_loss: 0.6819\n",
      "Epoch 33/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6818 - val_loss: 0.6815\n",
      "Epoch 34/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6814 - val_loss: 0.6811\n",
      "Epoch 35/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6810 - val_loss: 0.6807\n",
      "Epoch 36/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6806 - val_loss: 0.6803\n",
      "Epoch 37/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6802 - val_loss: 0.6799\n",
      "Epoch 38/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6797 - val_loss: 0.6795\n",
      "Epoch 39/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6793 - val_loss: 0.6791\n",
      "Epoch 40/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6789 - val_loss: 0.6786\n",
      "Epoch 41/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6785 - val_loss: 0.6782\n",
      "Epoch 42/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6781 - val_loss: 0.6778\n",
      "Epoch 43/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6777 - val_loss: 0.6774\n",
      "Epoch 44/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6773 - val_loss: 0.6770\n",
      "Epoch 45/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6769 - val_loss: 0.6766\n",
      "Epoch 46/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6764 - val_loss: 0.6762\n",
      "Epoch 47/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6760 - val_loss: 0.6758\n",
      "Epoch 48/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6756 - val_loss: 0.6753\n",
      "Epoch 49/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6752 - val_loss: 0.6749\n",
      "Epoch 50/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6748 - val_loss: 0.6745\n",
      "Epoch 51/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6744 - val_loss: 0.6741\n",
      "Epoch 52/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6739 - val_loss: 0.6737\n",
      "Epoch 53/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6735 - val_loss: 0.6733\n",
      "Epoch 54/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6731 - val_loss: 0.6728\n",
      "Epoch 55/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6727 - val_loss: 0.6724\n",
      "Epoch 56/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6723 - val_loss: 0.6720\n",
      "Epoch 57/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6718 - val_loss: 0.6716\n",
      "Epoch 58/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6714 - val_loss: 0.6711\n",
      "Epoch 59/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6710 - val_loss: 0.6707\n",
      "Epoch 60/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6706 - val_loss: 0.6703\n",
      "Epoch 61/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6702 - val_loss: 0.6699\n",
      "Epoch 62/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6697 - val_loss: 0.6695\n",
      "Epoch 63/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6693 - val_loss: 0.6690\n",
      "Epoch 64/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6689 - val_loss: 0.6686\n",
      "Epoch 65/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6684 - val_loss: 0.6682\n",
      "Epoch 66/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6680 - val_loss: 0.6677\n",
      "Epoch 67/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6676 - val_loss: 0.6673\n",
      "Epoch 68/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6672 - val_loss: 0.6669\n",
      "Epoch 69/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6667 - val_loss: 0.6664\n",
      "Epoch 70/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6663 - val_loss: 0.6660\n",
      "Epoch 71/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6659 - val_loss: 0.6656\n",
      "Epoch 72/1000\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.6654 - val_loss: 0.6651\n",
      "Epoch 73/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.6650 - val_loss: 0.6647\n",
      "Epoch 74/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6646 - val_loss: 0.6643\n",
      "Epoch 75/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6641 - val_loss: 0.6638\n",
      "Epoch 76/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6637 - val_loss: 0.6634\n",
      "Epoch 77/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6632 - val_loss: 0.6630\n",
      "Epoch 78/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6628 - val_loss: 0.6625\n",
      "Epoch 79/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6624 - val_loss: 0.6621\n",
      "Epoch 80/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6619 - val_loss: 0.6616\n",
      "Epoch 81/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6615 - val_loss: 0.6612\n",
      "Epoch 82/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6610 - val_loss: 0.6607\n",
      "Epoch 83/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6606 - val_loss: 0.6603\n",
      "Epoch 84/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6601 - val_loss: 0.6598\n",
      "Epoch 85/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6597 - val_loss: 0.6594\n",
      "Epoch 86/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6592 - val_loss: 0.6589\n",
      "Epoch 87/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6588 - val_loss: 0.6585\n",
      "Epoch 88/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6583 - val_loss: 0.6580\n",
      "Epoch 89/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6579 - val_loss: 0.6576\n",
      "Epoch 90/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6574 - val_loss: 0.6571\n",
      "Epoch 91/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6570 - val_loss: 0.6566\n",
      "Epoch 92/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6565 - val_loss: 0.6562\n",
      "Epoch 93/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6560 - val_loss: 0.6557\n",
      "Epoch 94/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6556 - val_loss: 0.6553\n",
      "Epoch 95/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6551 - val_loss: 0.6548\n",
      "Epoch 96/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6546 - val_loss: 0.6543\n",
      "Epoch 97/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6542 - val_loss: 0.6539\n",
      "Epoch 98/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6537 - val_loss: 0.6534\n",
      "Epoch 99/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6532 - val_loss: 0.6529\n",
      "Epoch 100/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6528 - val_loss: 0.6524\n",
      "Epoch 101/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6523 - val_loss: 0.6520\n",
      "Epoch 102/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6518 - val_loss: 0.6515\n",
      "Epoch 103/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6513 - val_loss: 0.6510\n",
      "Epoch 104/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6509 - val_loss: 0.6505\n",
      "Epoch 105/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6504 - val_loss: 0.6501\n",
      "Epoch 106/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6499 - val_loss: 0.6496\n",
      "Epoch 107/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6494 - val_loss: 0.6491\n",
      "Epoch 108/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6489 - val_loss: 0.6486\n",
      "Epoch 109/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6485 - val_loss: 0.6481\n",
      "Epoch 110/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6480 - val_loss: 0.6476\n",
      "Epoch 111/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6475 - val_loss: 0.6471\n",
      "Epoch 112/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6470 - val_loss: 0.6466\n",
      "Epoch 113/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6465 - val_loss: 0.6462\n",
      "Epoch 114/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6460 - val_loss: 0.6457\n",
      "Epoch 115/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6455 - val_loss: 0.6452\n",
      "Epoch 116/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6450 - val_loss: 0.6447\n",
      "Epoch 117/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6445 - val_loss: 0.6442\n",
      "Epoch 118/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6440 - val_loss: 0.6437\n",
      "Epoch 119/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6435 - val_loss: 0.6432\n",
      "Epoch 120/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6430 - val_loss: 0.6426\n",
      "Epoch 121/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6425 - val_loss: 0.6421\n",
      "Epoch 122/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6420 - val_loss: 0.6416\n",
      "Epoch 123/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6415 - val_loss: 0.6411\n",
      "Epoch 124/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6410 - val_loss: 0.6406\n",
      "Epoch 125/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6405 - val_loss: 0.6401\n",
      "Epoch 126/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6400 - val_loss: 0.6396\n",
      "Epoch 127/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6394 - val_loss: 0.6391\n",
      "Epoch 128/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6389 - val_loss: 0.6385\n",
      "Epoch 129/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6384 - val_loss: 0.6380\n",
      "Epoch 130/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6379 - val_loss: 0.6375\n",
      "Epoch 131/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6374 - val_loss: 0.6370\n",
      "Epoch 132/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6368 - val_loss: 0.6364\n",
      "Epoch 133/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6363 - val_loss: 0.6359\n",
      "Epoch 134/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6358 - val_loss: 0.6354\n",
      "Epoch 135/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6352 - val_loss: 0.6348\n",
      "Epoch 136/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6347 - val_loss: 0.6343\n",
      "Epoch 137/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6342 - val_loss: 0.6338\n",
      "Epoch 138/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6336 - val_loss: 0.6332\n",
      "Epoch 139/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6331 - val_loss: 0.6327\n",
      "Epoch 140/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6326 - val_loss: 0.6322\n",
      "Epoch 141/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6320 - val_loss: 0.6316\n",
      "Epoch 142/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6315 - val_loss: 0.6311\n",
      "Epoch 143/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6309 - val_loss: 0.6305\n",
      "Epoch 144/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6304 - val_loss: 0.6300\n",
      "Epoch 145/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6298 - val_loss: 0.6294\n",
      "Epoch 146/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6293 - val_loss: 0.6289\n",
      "Epoch 147/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6287 - val_loss: 0.6283\n",
      "Epoch 148/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6282 - val_loss: 0.6278\n",
      "Epoch 149/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6276 - val_loss: 0.6272\n",
      "Epoch 150/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6271 - val_loss: 0.6266\n",
      "Epoch 151/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6265 - val_loss: 0.6261\n",
      "Epoch 152/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6260 - val_loss: 0.6255\n",
      "Epoch 153/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6254 - val_loss: 0.6249\n",
      "Epoch 154/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6248 - val_loss: 0.6244\n",
      "Epoch 155/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6243 - val_loss: 0.6238\n",
      "Epoch 156/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6237 - val_loss: 0.6232\n",
      "Epoch 157/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6231 - val_loss: 0.6227\n",
      "Epoch 158/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6226 - val_loss: 0.6221\n",
      "Epoch 159/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6220 - val_loss: 0.6215\n",
      "Epoch 160/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6214 - val_loss: 0.6209\n",
      "Epoch 161/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6208 - val_loss: 0.6204\n",
      "Epoch 162/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6203 - val_loss: 0.6198\n",
      "Epoch 163/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6197 - val_loss: 0.6192\n",
      "Epoch 164/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6191 - val_loss: 0.6186\n",
      "Epoch 165/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6185 - val_loss: 0.6180\n",
      "Epoch 166/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6179 - val_loss: 0.6174\n",
      "Epoch 167/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6173 - val_loss: 0.6169\n",
      "Epoch 168/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6168 - val_loss: 0.6163\n",
      "Epoch 169/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6162 - val_loss: 0.6157\n",
      "Epoch 170/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6156 - val_loss: 0.6151\n",
      "Epoch 171/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6150 - val_loss: 0.6145\n",
      "Epoch 172/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.6144 - val_loss: 0.6139\n",
      "Epoch 173/1000\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.6138 - val_loss: 0.6133\n",
      "Epoch 174/1000\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.6132 - val_loss: 0.6127\n",
      "Epoch 175/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6126 - val_loss: 0.6121\n",
      "Epoch 176/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6120 - val_loss: 0.6115\n",
      "Epoch 177/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6114 - val_loss: 0.6109\n",
      "Epoch 178/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.6108 - val_loss: 0.6103\n",
      "Epoch 179/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6102 - val_loss: 0.6097\n",
      "Epoch 180/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6096 - val_loss: 0.6091\n",
      "Epoch 181/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6090 - val_loss: 0.6084\n",
      "Epoch 182/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.6084 - val_loss: 0.6078\n",
      "Epoch 183/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6078 - val_loss: 0.6072\n",
      "Epoch 184/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6072 - val_loss: 0.6066\n",
      "Epoch 185/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6065 - val_loss: 0.6060\n",
      "Epoch 186/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6059 - val_loss: 0.6054\n",
      "Epoch 187/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6053 - val_loss: 0.6047\n",
      "Epoch 188/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6047 - val_loss: 0.6041\n",
      "Epoch 189/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6041 - val_loss: 0.6035\n",
      "Epoch 190/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6035 - val_loss: 0.6029\n",
      "Epoch 191/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6028 - val_loss: 0.6023\n",
      "Epoch 192/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6022 - val_loss: 0.6016\n",
      "Epoch 193/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6016 - val_loss: 0.6010\n",
      "Epoch 194/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6010 - val_loss: 0.6004\n",
      "Epoch 195/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6003 - val_loss: 0.5997\n",
      "Epoch 196/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.5997 - val_loss: 0.5991\n",
      "Epoch 197/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.5991 - val_loss: 0.5985\n",
      "Epoch 198/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5985 - val_loss: 0.5979\n",
      "Epoch 199/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5978 - val_loss: 0.5972\n",
      "Epoch 200/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5972 - val_loss: 0.5966\n",
      "Epoch 201/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5966 - val_loss: 0.5959\n",
      "Epoch 202/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5959 - val_loss: 0.5953\n",
      "Epoch 203/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5953 - val_loss: 0.5947\n",
      "Epoch 204/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5946 - val_loss: 0.5940\n",
      "Epoch 205/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5940 - val_loss: 0.5934\n",
      "Epoch 206/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5934 - val_loss: 0.5927\n",
      "Epoch 207/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5927 - val_loss: 0.5921\n",
      "Epoch 208/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5921 - val_loss: 0.5915\n",
      "Epoch 209/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5915 - val_loss: 0.5908\n",
      "Epoch 210/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5908 - val_loss: 0.5902\n",
      "Epoch 211/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5902 - val_loss: 0.5895\n",
      "Epoch 212/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5895 - val_loss: 0.5889\n",
      "Epoch 213/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5889 - val_loss: 0.5882\n",
      "Epoch 214/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5882 - val_loss: 0.5876\n",
      "Epoch 215/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5876 - val_loss: 0.5869\n",
      "Epoch 216/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5869 - val_loss: 0.5863\n",
      "Epoch 217/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5863 - val_loss: 0.5856\n",
      "Epoch 218/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5856 - val_loss: 0.5850\n",
      "Epoch 219/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5850 - val_loss: 0.5843\n",
      "Epoch 220/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5843 - val_loss: 0.5837\n",
      "Epoch 221/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5837 - val_loss: 0.5830\n",
      "Epoch 222/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5830 - val_loss: 0.5824\n",
      "Epoch 223/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5824 - val_loss: 0.5817\n",
      "Epoch 224/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5817 - val_loss: 0.5810\n",
      "Epoch 225/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5811 - val_loss: 0.5804\n",
      "Epoch 226/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5804 - val_loss: 0.5797\n",
      "Epoch 227/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5798 - val_loss: 0.5791\n",
      "Epoch 228/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5791 - val_loss: 0.5784\n",
      "Epoch 229/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5785 - val_loss: 0.5778\n",
      "Epoch 230/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5778 - val_loss: 0.5771\n",
      "Epoch 231/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5771 - val_loss: 0.5764\n",
      "Epoch 232/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5765 - val_loss: 0.5758\n",
      "Epoch 233/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5758 - val_loss: 0.5751\n",
      "Epoch 234/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5752 - val_loss: 0.5745\n",
      "Epoch 235/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5745 - val_loss: 0.5738\n",
      "Epoch 236/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5738 - val_loss: 0.5731\n",
      "Epoch 237/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5732 - val_loss: 0.5725\n",
      "Epoch 238/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5725 - val_loss: 0.5718\n",
      "Epoch 239/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5719 - val_loss: 0.5711\n",
      "Epoch 240/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5712 - val_loss: 0.5705\n",
      "Epoch 241/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5705 - val_loss: 0.5698\n",
      "Epoch 242/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5699 - val_loss: 0.5691\n",
      "Epoch 243/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5692 - val_loss: 0.5685\n",
      "Epoch 244/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5686 - val_loss: 0.5678\n",
      "Epoch 245/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5679 - val_loss: 0.5672\n",
      "Epoch 246/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5672 - val_loss: 0.5665\n",
      "Epoch 247/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5666 - val_loss: 0.5658\n",
      "Epoch 248/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5659 - val_loss: 0.5652\n",
      "Epoch 249/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5652 - val_loss: 0.5645\n",
      "Epoch 250/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5646 - val_loss: 0.5638\n",
      "Epoch 251/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5639 - val_loss: 0.5632\n",
      "Epoch 252/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5633 - val_loss: 0.5625\n",
      "Epoch 253/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5626 - val_loss: 0.5618\n",
      "Epoch 254/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5619 - val_loss: 0.5612\n",
      "Epoch 255/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5613 - val_loss: 0.5605\n",
      "Epoch 256/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5606 - val_loss: 0.5598\n",
      "Epoch 257/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5599 - val_loss: 0.5592\n",
      "Epoch 258/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5593 - val_loss: 0.5585\n",
      "Epoch 259/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5586 - val_loss: 0.5578\n",
      "Epoch 260/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5579 - val_loss: 0.5572\n",
      "Epoch 261/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5573 - val_loss: 0.5565\n",
      "Epoch 262/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5566 - val_loss: 0.5558\n",
      "Epoch 263/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5560 - val_loss: 0.5552\n",
      "Epoch 264/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5553 - val_loss: 0.5545\n",
      "Epoch 265/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5546 - val_loss: 0.5539\n",
      "Epoch 266/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5540 - val_loss: 0.5532\n",
      "Epoch 267/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5533 - val_loss: 0.5525\n",
      "Epoch 268/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5527 - val_loss: 0.5519\n",
      "Epoch 269/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5520 - val_loss: 0.5512\n",
      "Epoch 270/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5513 - val_loss: 0.5505\n",
      "Epoch 271/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5507 - val_loss: 0.5499\n",
      "Epoch 272/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5500 - val_loss: 0.5492\n",
      "Epoch 273/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.5494 - val_loss: 0.5486\n",
      "Epoch 274/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.5487 - val_loss: 0.5479\n",
      "Epoch 275/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.5480 - val_loss: 0.5472\n",
      "Epoch 276/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5474 - val_loss: 0.5466\n",
      "Epoch 277/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5467 - val_loss: 0.5459\n",
      "Epoch 278/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5461 - val_loss: 0.5453\n",
      "Epoch 279/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5454 - val_loss: 0.5446\n",
      "Epoch 280/1000\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.5448 - val_loss: 0.5439\n",
      "Epoch 281/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5441 - val_loss: 0.5433\n",
      "Epoch 282/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5434 - val_loss: 0.5426\n",
      "Epoch 283/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5428 - val_loss: 0.5420\n",
      "Epoch 284/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5421 - val_loss: 0.5413\n",
      "Epoch 285/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5415 - val_loss: 0.5407\n",
      "Epoch 286/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5408 - val_loss: 0.5400\n",
      "Epoch 287/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5402 - val_loss: 0.5394\n",
      "Epoch 288/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5395 - val_loss: 0.5387\n",
      "Epoch 289/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5389 - val_loss: 0.5380\n",
      "Epoch 290/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5382 - val_loss: 0.5374\n",
      "Epoch 291/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5376 - val_loss: 0.5367\n",
      "Epoch 292/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5369 - val_loss: 0.5361\n",
      "Epoch 293/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5363 - val_loss: 0.5354\n",
      "Epoch 294/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5356 - val_loss: 0.5348\n",
      "Epoch 295/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5350 - val_loss: 0.5341\n",
      "Epoch 296/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5343 - val_loss: 0.5335\n",
      "Epoch 297/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5337 - val_loss: 0.5329\n",
      "Epoch 298/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5330 - val_loss: 0.5322\n",
      "Epoch 299/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5324 - val_loss: 0.5316\n",
      "Epoch 300/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5318 - val_loss: 0.5309\n",
      "Epoch 301/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5311 - val_loss: 0.5303\n",
      "Epoch 302/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5305 - val_loss: 0.5296\n",
      "Epoch 303/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5298 - val_loss: 0.5290\n",
      "Epoch 304/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5292 - val_loss: 0.5283\n",
      "Epoch 305/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5286 - val_loss: 0.5277\n",
      "Epoch 306/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5279 - val_loss: 0.5271\n",
      "Epoch 307/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5273 - val_loss: 0.5264\n",
      "Epoch 308/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5266 - val_loss: 0.5258\n",
      "Epoch 309/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5260 - val_loss: 0.5252\n",
      "Epoch 310/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5254 - val_loss: 0.5245\n",
      "Epoch 311/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5247 - val_loss: 0.5239\n",
      "Epoch 312/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5241 - val_loss: 0.5233\n",
      "Epoch 313/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5235 - val_loss: 0.5226\n",
      "Epoch 314/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5228 - val_loss: 0.5220\n",
      "Epoch 315/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5222 - val_loss: 0.5214\n",
      "Epoch 316/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5216 - val_loss: 0.5207\n",
      "Epoch 317/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5210 - val_loss: 0.5201\n",
      "Epoch 318/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5203 - val_loss: 0.5195\n",
      "Epoch 319/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5197 - val_loss: 0.5189\n",
      "Epoch 320/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5191 - val_loss: 0.5182\n",
      "Epoch 321/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5185 - val_loss: 0.5176\n",
      "Epoch 322/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5178 - val_loss: 0.5170\n",
      "Epoch 323/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5172 - val_loss: 0.5164\n",
      "Epoch 324/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5166 - val_loss: 0.5157\n",
      "Epoch 325/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5160 - val_loss: 0.5151\n",
      "Epoch 326/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5154 - val_loss: 0.5145\n",
      "Epoch 327/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5148 - val_loss: 0.5139\n",
      "Epoch 328/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5141 - val_loss: 0.5133\n",
      "Epoch 329/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5135 - val_loss: 0.5127\n",
      "Epoch 330/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5129 - val_loss: 0.5120\n",
      "Epoch 331/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5123 - val_loss: 0.5114\n",
      "Epoch 332/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5117 - val_loss: 0.5108\n",
      "Epoch 333/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5111 - val_loss: 0.5102\n",
      "Epoch 334/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5105 - val_loss: 0.5096\n",
      "Epoch 335/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5099 - val_loss: 0.5090\n",
      "Epoch 336/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5093 - val_loss: 0.5084\n",
      "Epoch 337/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5087 - val_loss: 0.5078\n",
      "Epoch 338/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5081 - val_loss: 0.5072\n",
      "Epoch 339/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5075 - val_loss: 0.5066\n",
      "Epoch 340/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5069 - val_loss: 0.5060\n",
      "Epoch 341/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5063 - val_loss: 0.5054\n",
      "Epoch 342/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5057 - val_loss: 0.5048\n",
      "Epoch 343/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5051 - val_loss: 0.5042\n",
      "Epoch 344/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5045 - val_loss: 0.5036\n",
      "Epoch 345/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5039 - val_loss: 0.5030\n",
      "Epoch 346/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5033 - val_loss: 0.5024\n",
      "Epoch 347/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5027 - val_loss: 0.5018\n",
      "Epoch 348/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.5021 - val_loss: 0.5012\n",
      "Epoch 349/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5015 - val_loss: 0.5006\n",
      "Epoch 350/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5009 - val_loss: 0.5001\n",
      "Epoch 351/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.5003 - val_loss: 0.4995\n",
      "Epoch 352/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4997 - val_loss: 0.4989\n",
      "Epoch 353/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4992 - val_loss: 0.4983\n",
      "Epoch 354/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4986 - val_loss: 0.4977\n",
      "Epoch 355/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4980 - val_loss: 0.4971\n",
      "Epoch 356/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4974 - val_loss: 0.4966\n",
      "Epoch 357/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4968 - val_loss: 0.4960\n",
      "Epoch 358/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4963 - val_loss: 0.4954\n",
      "Epoch 359/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4957 - val_loss: 0.4948\n",
      "Epoch 360/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4951 - val_loss: 0.4943\n",
      "Epoch 361/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4945 - val_loss: 0.4937\n",
      "Epoch 362/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4940 - val_loss: 0.4931\n",
      "Epoch 363/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4934 - val_loss: 0.4925\n",
      "Epoch 364/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4928 - val_loss: 0.4920\n",
      "Epoch 365/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4923 - val_loss: 0.4914\n",
      "Epoch 366/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4917 - val_loss: 0.4908\n",
      "Epoch 367/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4911 - val_loss: 0.4903\n",
      "Epoch 368/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4906 - val_loss: 0.4897\n",
      "Epoch 369/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4900 - val_loss: 0.4891\n",
      "Epoch 370/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4895 - val_loss: 0.4886\n",
      "Epoch 371/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4889 - val_loss: 0.4880\n",
      "Epoch 372/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4883 - val_loss: 0.4875\n",
      "Epoch 373/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4878 - val_loss: 0.4869\n",
      "Epoch 374/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4872 - val_loss: 0.4864\n",
      "Epoch 375/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4867 - val_loss: 0.4858\n",
      "Epoch 376/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.4861 - val_loss: 0.4853\n",
      "Epoch 377/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.4856 - val_loss: 0.4847\n",
      "Epoch 378/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4850 - val_loss: 0.4842\n",
      "Epoch 379/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4845 - val_loss: 0.4836\n",
      "Epoch 380/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4839 - val_loss: 0.4831\n",
      "Epoch 381/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4834 - val_loss: 0.4825\n",
      "Epoch 382/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4828 - val_loss: 0.4820\n",
      "Epoch 383/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4823 - val_loss: 0.4814\n",
      "Epoch 384/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4818 - val_loss: 0.4809\n",
      "Epoch 385/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4812 - val_loss: 0.4804\n",
      "Epoch 386/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4807 - val_loss: 0.4798\n",
      "Epoch 387/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4802 - val_loss: 0.4793\n",
      "Epoch 388/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4796 - val_loss: 0.4788\n",
      "Epoch 389/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4791 - val_loss: 0.4782\n",
      "Epoch 390/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4786 - val_loss: 0.4777\n",
      "Epoch 391/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4780 - val_loss: 0.4772\n",
      "Epoch 392/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4775 - val_loss: 0.4766\n",
      "Epoch 393/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4770 - val_loss: 0.4761\n",
      "Epoch 394/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4764 - val_loss: 0.4756\n",
      "Epoch 395/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4759 - val_loss: 0.4751\n",
      "Epoch 396/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4754 - val_loss: 0.4745\n",
      "Epoch 397/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4749 - val_loss: 0.4740\n",
      "Epoch 398/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4744 - val_loss: 0.4735\n",
      "Epoch 399/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4738 - val_loss: 0.4730\n",
      "Epoch 400/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4733 - val_loss: 0.4725\n",
      "Epoch 401/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4728 - val_loss: 0.4720\n",
      "Epoch 402/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4723 - val_loss: 0.4714\n",
      "Epoch 403/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4718 - val_loss: 0.4709\n",
      "Epoch 404/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4713 - val_loss: 0.4704\n",
      "Epoch 405/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4708 - val_loss: 0.4699\n",
      "Epoch 406/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4703 - val_loss: 0.4694\n",
      "Epoch 407/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4697 - val_loss: 0.4689\n",
      "Epoch 408/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4692 - val_loss: 0.4684\n",
      "Epoch 409/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4687 - val_loss: 0.4679\n",
      "Epoch 410/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4682 - val_loss: 0.4674\n",
      "Epoch 411/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4677 - val_loss: 0.4669\n",
      "Epoch 412/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4672 - val_loss: 0.4664\n",
      "Epoch 413/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4667 - val_loss: 0.4659\n",
      "Epoch 414/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4662 - val_loss: 0.4654\n",
      "Epoch 415/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4658 - val_loss: 0.4649\n",
      "Epoch 416/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4653 - val_loss: 0.4644\n",
      "Epoch 417/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4648 - val_loss: 0.4639\n",
      "Epoch 418/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4643 - val_loss: 0.4634\n",
      "Epoch 419/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4638 - val_loss: 0.4630\n",
      "Epoch 420/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4633 - val_loss: 0.4625\n",
      "Epoch 421/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4628 - val_loss: 0.4620\n",
      "Epoch 422/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4623 - val_loss: 0.4615\n",
      "Epoch 423/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4618 - val_loss: 0.4610\n",
      "Epoch 424/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4614 - val_loss: 0.4605\n",
      "Epoch 425/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4609 - val_loss: 0.4601\n",
      "Epoch 426/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4604 - val_loss: 0.4596\n",
      "Epoch 427/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4599 - val_loss: 0.4591\n",
      "Epoch 428/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4595 - val_loss: 0.4586\n",
      "Epoch 429/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4590 - val_loss: 0.4582\n",
      "Epoch 430/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4585 - val_loss: 0.4577\n",
      "Epoch 431/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4580 - val_loss: 0.4572\n",
      "Epoch 432/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4576 - val_loss: 0.4567\n",
      "Epoch 433/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4571 - val_loss: 0.4563\n",
      "Epoch 434/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4566 - val_loss: 0.4558\n",
      "Epoch 435/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4562 - val_loss: 0.4554\n",
      "Epoch 436/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4557 - val_loss: 0.4549\n",
      "Epoch 437/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4552 - val_loss: 0.4544\n",
      "Epoch 438/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4548 - val_loss: 0.4540\n",
      "Epoch 439/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4543 - val_loss: 0.4535\n",
      "Epoch 440/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4539 - val_loss: 0.4531\n",
      "Epoch 441/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4534 - val_loss: 0.4526\n",
      "Epoch 442/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4529 - val_loss: 0.4521\n",
      "Epoch 443/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4525 - val_loss: 0.4517\n",
      "Epoch 444/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4520 - val_loss: 0.4512\n",
      "Epoch 445/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4516 - val_loss: 0.4508\n",
      "Epoch 446/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4511 - val_loss: 0.4503\n",
      "Epoch 447/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4507 - val_loss: 0.4499\n",
      "Epoch 448/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4502 - val_loss: 0.4494\n",
      "Epoch 449/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4498 - val_loss: 0.4490\n",
      "Epoch 450/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4494 - val_loss: 0.4486\n",
      "Epoch 451/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4489 - val_loss: 0.4481\n",
      "Epoch 452/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4485 - val_loss: 0.4477\n",
      "Epoch 453/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4480 - val_loss: 0.4472\n",
      "Epoch 454/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4476 - val_loss: 0.4468\n",
      "Epoch 455/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4472 - val_loss: 0.4464\n",
      "Epoch 456/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4467 - val_loss: 0.4459\n",
      "Epoch 457/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4463 - val_loss: 0.4455\n",
      "Epoch 458/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4459 - val_loss: 0.4451\n",
      "Epoch 459/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4454 - val_loss: 0.4446\n",
      "Epoch 460/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4450 - val_loss: 0.4442\n",
      "Epoch 461/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4446 - val_loss: 0.4438\n",
      "Epoch 462/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4441 - val_loss: 0.4433\n",
      "Epoch 463/1000\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.4437 - val_loss: 0.4429\n",
      "Epoch 464/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4433 - val_loss: 0.4425\n",
      "Epoch 465/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4429 - val_loss: 0.4421\n",
      "Epoch 466/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4424 - val_loss: 0.4417\n",
      "Epoch 467/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4420 - val_loss: 0.4412\n",
      "Epoch 468/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4416 - val_loss: 0.4408\n",
      "Epoch 469/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4412 - val_loss: 0.4404\n",
      "Epoch 470/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4408 - val_loss: 0.4400\n",
      "Epoch 471/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4403 - val_loss: 0.4396\n",
      "Epoch 472/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4399 - val_loss: 0.4392\n",
      "Epoch 473/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4395 - val_loss: 0.4387\n",
      "Epoch 474/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4391 - val_loss: 0.4383\n",
      "Epoch 475/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4387 - val_loss: 0.4379\n",
      "Epoch 476/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4383 - val_loss: 0.4375\n",
      "Epoch 477/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4379 - val_loss: 0.4371\n",
      "Epoch 478/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4375 - val_loss: 0.4367\n",
      "Epoch 479/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.4371 - val_loss: 0.4363\n",
      "Epoch 480/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.4367 - val_loss: 0.4359\n",
      "Epoch 481/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4363 - val_loss: 0.4355\n",
      "Epoch 482/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4359 - val_loss: 0.4351\n",
      "Epoch 483/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4355 - val_loss: 0.4347\n",
      "Epoch 484/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4351 - val_loss: 0.4343\n",
      "Epoch 485/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4347 - val_loss: 0.4339\n",
      "Epoch 486/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4343 - val_loss: 0.4335\n",
      "Epoch 487/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4339 - val_loss: 0.4331\n",
      "Epoch 488/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4335 - val_loss: 0.4327\n",
      "Epoch 489/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4331 - val_loss: 0.4323\n",
      "Epoch 490/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4327 - val_loss: 0.4319\n",
      "Epoch 491/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4323 - val_loss: 0.4316\n",
      "Epoch 492/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4319 - val_loss: 0.4312\n",
      "Epoch 493/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4315 - val_loss: 0.4308\n",
      "Epoch 494/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.4311 - val_loss: 0.4304\n",
      "Epoch 495/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4308 - val_loss: 0.4300\n",
      "Epoch 496/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4304 - val_loss: 0.4296\n",
      "Epoch 497/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4300 - val_loss: 0.4292\n",
      "Epoch 498/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4296 - val_loss: 0.4289\n",
      "Epoch 499/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4292 - val_loss: 0.4285\n",
      "Epoch 500/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4289 - val_loss: 0.4281\n",
      "Epoch 501/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4285 - val_loss: 0.4277\n",
      "Epoch 502/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4281 - val_loss: 0.4274\n",
      "Epoch 503/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4277 - val_loss: 0.4270\n",
      "Epoch 504/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4273 - val_loss: 0.4266\n",
      "Epoch 505/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4270 - val_loss: 0.4262\n",
      "Epoch 506/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4266 - val_loss: 0.4259\n",
      "Epoch 507/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4262 - val_loss: 0.4255\n",
      "Epoch 508/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4259 - val_loss: 0.4251\n",
      "Epoch 509/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4255 - val_loss: 0.4248\n",
      "Epoch 510/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4251 - val_loss: 0.4244\n",
      "Epoch 511/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4248 - val_loss: 0.4240\n",
      "Epoch 512/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4244 - val_loss: 0.4237\n",
      "Epoch 513/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4240 - val_loss: 0.4233\n",
      "Epoch 514/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4237 - val_loss: 0.4230\n",
      "Epoch 515/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4233 - val_loss: 0.4226\n",
      "Epoch 516/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4230 - val_loss: 0.4222\n",
      "Epoch 517/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4226 - val_loss: 0.4219\n",
      "Epoch 518/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4222 - val_loss: 0.4215\n",
      "Epoch 519/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4219 - val_loss: 0.4212\n",
      "Epoch 520/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4215 - val_loss: 0.4208\n",
      "Epoch 521/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4212 - val_loss: 0.4205\n",
      "Epoch 522/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4208 - val_loss: 0.4201\n",
      "Epoch 523/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4205 - val_loss: 0.4198\n",
      "Epoch 524/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4201 - val_loss: 0.4194\n",
      "Epoch 525/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4198 - val_loss: 0.4191\n",
      "Epoch 526/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4194 - val_loss: 0.4187\n",
      "Epoch 527/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4191 - val_loss: 0.4184\n",
      "Epoch 528/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4187 - val_loss: 0.4180\n",
      "Epoch 529/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4184 - val_loss: 0.4177\n",
      "Epoch 530/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4180 - val_loss: 0.4173\n",
      "Epoch 531/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4177 - val_loss: 0.4170\n",
      "Epoch 532/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4174 - val_loss: 0.4167\n",
      "Epoch 533/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4170 - val_loss: 0.4163\n",
      "Epoch 534/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4167 - val_loss: 0.4160\n",
      "Epoch 535/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4163 - val_loss: 0.4157\n",
      "Epoch 536/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4160 - val_loss: 0.4153\n",
      "Epoch 537/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4157 - val_loss: 0.4150\n",
      "Epoch 538/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4153 - val_loss: 0.4147\n",
      "Epoch 539/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4150 - val_loss: 0.4143\n",
      "Epoch 540/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4147 - val_loss: 0.4140\n",
      "Epoch 541/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4143 - val_loss: 0.4137\n",
      "Epoch 542/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4140 - val_loss: 0.4133\n",
      "Epoch 543/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4137 - val_loss: 0.4130\n",
      "Epoch 544/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4134 - val_loss: 0.4127\n",
      "Epoch 545/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4130 - val_loss: 0.4123\n",
      "Epoch 546/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4127 - val_loss: 0.4120\n",
      "Epoch 547/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4124 - val_loss: 0.4117\n",
      "Epoch 548/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4121 - val_loss: 0.4114\n",
      "Epoch 549/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4117 - val_loss: 0.4111\n",
      "Epoch 550/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4114 - val_loss: 0.4107\n",
      "Epoch 551/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4111 - val_loss: 0.4104\n",
      "Epoch 552/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4108 - val_loss: 0.4101\n",
      "Epoch 553/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4104 - val_loss: 0.4098\n",
      "Epoch 554/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4101 - val_loss: 0.4095\n",
      "Epoch 555/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4098 - val_loss: 0.4091\n",
      "Epoch 556/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4095 - val_loss: 0.4088\n",
      "Epoch 557/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4092 - val_loss: 0.4085\n",
      "Epoch 558/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4089 - val_loss: 0.4082\n",
      "Epoch 559/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4086 - val_loss: 0.4079\n",
      "Epoch 560/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4082 - val_loss: 0.4076\n",
      "Epoch 561/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4079 - val_loss: 0.4073\n",
      "Epoch 562/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4076 - val_loss: 0.4070\n",
      "Epoch 563/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4073 - val_loss: 0.4067\n",
      "Epoch 564/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4070 - val_loss: 0.4064\n",
      "Epoch 565/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4067 - val_loss: 0.4061\n",
      "Epoch 566/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4064 - val_loss: 0.4057\n",
      "Epoch 567/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4061 - val_loss: 0.4054\n",
      "Epoch 568/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4058 - val_loss: 0.4051\n",
      "Epoch 569/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4055 - val_loss: 0.4048\n",
      "Epoch 570/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4052 - val_loss: 0.4045\n",
      "Epoch 571/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4049 - val_loss: 0.4042\n",
      "Epoch 572/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4046 - val_loss: 0.4039\n",
      "Epoch 573/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4043 - val_loss: 0.4036\n",
      "Epoch 574/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4040 - val_loss: 0.4033\n",
      "Epoch 575/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4037 - val_loss: 0.4031\n",
      "Epoch 576/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4034 - val_loss: 0.4028\n",
      "Epoch 577/1000\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.4031 - val_loss: 0.4025\n",
      "Epoch 578/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.4028 - val_loss: 0.4022\n",
      "Epoch 579/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.4025 - val_loss: 0.4019\n",
      "Epoch 580/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4022 - val_loss: 0.4016\n",
      "Epoch 581/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4019 - val_loss: 0.4013\n",
      "Epoch 582/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.4016 - val_loss: 0.4010\n",
      "Epoch 583/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4013 - val_loss: 0.4007\n",
      "Epoch 584/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4011 - val_loss: 0.4004\n",
      "Epoch 585/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4008 - val_loss: 0.4001\n",
      "Epoch 586/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4005 - val_loss: 0.3999\n",
      "Epoch 587/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.4002 - val_loss: 0.3996\n",
      "Epoch 588/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3999 - val_loss: 0.3993\n",
      "Epoch 589/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3996 - val_loss: 0.3990\n",
      "Epoch 590/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3993 - val_loss: 0.3987\n",
      "Epoch 591/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3991 - val_loss: 0.3984\n",
      "Epoch 592/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3988 - val_loss: 0.3982\n",
      "Epoch 593/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3985 - val_loss: 0.3979\n",
      "Epoch 594/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3982 - val_loss: 0.3976\n",
      "Epoch 595/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3979 - val_loss: 0.3973\n",
      "Epoch 596/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3977 - val_loss: 0.3971\n",
      "Epoch 597/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3974 - val_loss: 0.3968\n",
      "Epoch 598/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3971 - val_loss: 0.3965\n",
      "Epoch 599/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3968 - val_loss: 0.3962\n",
      "Epoch 600/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3966 - val_loss: 0.3960\n",
      "Epoch 601/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3963 - val_loss: 0.3957\n",
      "Epoch 602/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3960 - val_loss: 0.3954\n",
      "Epoch 603/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3958 - val_loss: 0.3951\n",
      "Epoch 604/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3955 - val_loss: 0.3949\n",
      "Epoch 605/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3952 - val_loss: 0.3946\n",
      "Epoch 606/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3949 - val_loss: 0.3943\n",
      "Epoch 607/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3947 - val_loss: 0.3941\n",
      "Epoch 608/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3944 - val_loss: 0.3938\n",
      "Epoch 609/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3941 - val_loss: 0.3935\n",
      "Epoch 610/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3939 - val_loss: 0.3933\n",
      "Epoch 611/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3936 - val_loss: 0.3930\n",
      "Epoch 612/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3933 - val_loss: 0.3928\n",
      "Epoch 613/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3931 - val_loss: 0.3925\n",
      "Epoch 614/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3928 - val_loss: 0.3922\n",
      "Epoch 615/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3926 - val_loss: 0.3920\n",
      "Epoch 616/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3923 - val_loss: 0.3917\n",
      "Epoch 617/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3920 - val_loss: 0.3915\n",
      "Epoch 618/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3918 - val_loss: 0.3912\n",
      "Epoch 619/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3915 - val_loss: 0.3909\n",
      "Epoch 620/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3913 - val_loss: 0.3907\n",
      "Epoch 621/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3910 - val_loss: 0.3904\n",
      "Epoch 622/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3908 - val_loss: 0.3902\n",
      "Epoch 623/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3905 - val_loss: 0.3899\n",
      "Epoch 624/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3902 - val_loss: 0.3897\n",
      "Epoch 625/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3900 - val_loss: 0.3894\n",
      "Epoch 626/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3897 - val_loss: 0.3892\n",
      "Epoch 627/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3895 - val_loss: 0.3889\n",
      "Epoch 628/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3892 - val_loss: 0.3887\n",
      "Epoch 629/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3890 - val_loss: 0.3884\n",
      "Epoch 630/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3887 - val_loss: 0.3882\n",
      "Epoch 631/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3885 - val_loss: 0.3879\n",
      "Epoch 632/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3882 - val_loss: 0.3877\n",
      "Epoch 633/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3880 - val_loss: 0.3874\n",
      "Epoch 634/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3877 - val_loss: 0.3872\n",
      "Epoch 635/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3875 - val_loss: 0.3869\n",
      "Epoch 636/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3873 - val_loss: 0.3867\n",
      "Epoch 637/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3870 - val_loss: 0.3864\n",
      "Epoch 638/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3868 - val_loss: 0.3862\n",
      "Epoch 639/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3865 - val_loss: 0.3860\n",
      "Epoch 640/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3863 - val_loss: 0.3857\n",
      "Epoch 641/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3860 - val_loss: 0.3855\n",
      "Epoch 642/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3858 - val_loss: 0.3852\n",
      "Epoch 643/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3856 - val_loss: 0.3850\n",
      "Epoch 644/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3853 - val_loss: 0.3848\n",
      "Epoch 645/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3851 - val_loss: 0.3845\n",
      "Epoch 646/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3848 - val_loss: 0.3843\n",
      "Epoch 647/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3846 - val_loss: 0.3841\n",
      "Epoch 648/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3844 - val_loss: 0.3838\n",
      "Epoch 649/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3841 - val_loss: 0.3836\n",
      "Epoch 650/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3839 - val_loss: 0.3834\n",
      "Epoch 651/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3837 - val_loss: 0.3831\n",
      "Epoch 652/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3834 - val_loss: 0.3829\n",
      "Epoch 653/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3832 - val_loss: 0.3827\n",
      "Epoch 654/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3830 - val_loss: 0.3824\n",
      "Epoch 655/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3827 - val_loss: 0.3822\n",
      "Epoch 656/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3825 - val_loss: 0.3820\n",
      "Epoch 657/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3823 - val_loss: 0.3817\n",
      "Epoch 658/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3821 - val_loss: 0.3815\n",
      "Epoch 659/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3818 - val_loss: 0.3813\n",
      "Epoch 660/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3816 - val_loss: 0.3811\n",
      "Epoch 661/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3814 - val_loss: 0.3808\n",
      "Epoch 662/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3811 - val_loss: 0.3806\n",
      "Epoch 663/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3809 - val_loss: 0.3804\n",
      "Epoch 664/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3807 - val_loss: 0.3802\n",
      "Epoch 665/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3805 - val_loss: 0.3799\n",
      "Epoch 666/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3802 - val_loss: 0.3797\n",
      "Epoch 667/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3800 - val_loss: 0.3795\n",
      "Epoch 668/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3798 - val_loss: 0.3793\n",
      "Epoch 669/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3796 - val_loss: 0.3791\n",
      "Epoch 670/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3794 - val_loss: 0.3788\n",
      "Epoch 671/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3791 - val_loss: 0.3786\n",
      "Epoch 672/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3789 - val_loss: 0.3784\n",
      "Epoch 673/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3787 - val_loss: 0.3782\n",
      "Epoch 674/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3785 - val_loss: 0.3780\n",
      "Epoch 675/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3783 - val_loss: 0.3777\n",
      "Epoch 676/1000\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.3781 - val_loss: 0.3775\n",
      "Epoch 677/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3778 - val_loss: 0.3773\n",
      "Epoch 678/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3776 - val_loss: 0.3771\n",
      "Epoch 679/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3774 - val_loss: 0.3769\n",
      "Epoch 680/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3772 - val_loss: 0.3767\n",
      "Epoch 681/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3770 - val_loss: 0.3765\n",
      "Epoch 682/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3768 - val_loss: 0.3763\n",
      "Epoch 683/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3766 - val_loss: 0.3760\n",
      "Epoch 684/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3763 - val_loss: 0.3758\n",
      "Epoch 685/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3761 - val_loss: 0.3756\n",
      "Epoch 686/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3759 - val_loss: 0.3754\n",
      "Epoch 687/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3757 - val_loss: 0.3752\n",
      "Epoch 688/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3755 - val_loss: 0.3750\n",
      "Epoch 689/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3753 - val_loss: 0.3748\n",
      "Epoch 690/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3751 - val_loss: 0.3746\n",
      "Epoch 691/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3749 - val_loss: 0.3744\n",
      "Epoch 692/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3747 - val_loss: 0.3742\n",
      "Epoch 693/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3745 - val_loss: 0.3740\n",
      "Epoch 694/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3743 - val_loss: 0.3738\n",
      "Epoch 695/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3741 - val_loss: 0.3736\n",
      "Epoch 696/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3739 - val_loss: 0.3734\n",
      "Epoch 697/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3736 - val_loss: 0.3732\n",
      "Epoch 698/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3734 - val_loss: 0.3729\n",
      "Epoch 699/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3732 - val_loss: 0.3727\n",
      "Epoch 700/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3730 - val_loss: 0.3725\n",
      "Epoch 701/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3728 - val_loss: 0.3723\n",
      "Epoch 702/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3726 - val_loss: 0.3721\n",
      "Epoch 703/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3724 - val_loss: 0.3719\n",
      "Epoch 704/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3722 - val_loss: 0.3718\n",
      "Epoch 705/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3720 - val_loss: 0.3716\n",
      "Epoch 706/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3718 - val_loss: 0.3714\n",
      "Epoch 707/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3716 - val_loss: 0.3712\n",
      "Epoch 708/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3714 - val_loss: 0.3710\n",
      "Epoch 709/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3713 - val_loss: 0.3708\n",
      "Epoch 710/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3711 - val_loss: 0.3706\n",
      "Epoch 711/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3709 - val_loss: 0.3704\n",
      "Epoch 712/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3707 - val_loss: 0.3702\n",
      "Epoch 713/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3705 - val_loss: 0.3700\n",
      "Epoch 714/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3703 - val_loss: 0.3698\n",
      "Epoch 715/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3701 - val_loss: 0.3696\n",
      "Epoch 716/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3699 - val_loss: 0.3694\n",
      "Epoch 717/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3697 - val_loss: 0.3692\n",
      "Epoch 718/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3695 - val_loss: 0.3690\n",
      "Epoch 719/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3693 - val_loss: 0.3688\n",
      "Epoch 720/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3691 - val_loss: 0.3687\n",
      "Epoch 721/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3689 - val_loss: 0.3685\n",
      "Epoch 722/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3688 - val_loss: 0.3683\n",
      "Epoch 723/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3686 - val_loss: 0.3681\n",
      "Epoch 724/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3684 - val_loss: 0.3679\n",
      "Epoch 725/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3682 - val_loss: 0.3677\n",
      "Epoch 726/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3680 - val_loss: 0.3675\n",
      "Epoch 727/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3678 - val_loss: 0.3673\n",
      "Epoch 728/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3676 - val_loss: 0.3672\n",
      "Epoch 729/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3674 - val_loss: 0.3670\n",
      "Epoch 730/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3673 - val_loss: 0.3668\n",
      "Epoch 731/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3671 - val_loss: 0.3666\n",
      "Epoch 732/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3669 - val_loss: 0.3664\n",
      "Epoch 733/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3667 - val_loss: 0.3662\n",
      "Epoch 734/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3665 - val_loss: 0.3661\n",
      "Epoch 735/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3663 - val_loss: 0.3659\n",
      "Epoch 736/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3662 - val_loss: 0.3657\n",
      "Epoch 737/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3660 - val_loss: 0.3655\n",
      "Epoch 738/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3658 - val_loss: 0.3653\n",
      "Epoch 739/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3656 - val_loss: 0.3652\n",
      "Epoch 740/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3654 - val_loss: 0.3650\n",
      "Epoch 741/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3653 - val_loss: 0.3648\n",
      "Epoch 742/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3651 - val_loss: 0.3646\n",
      "Epoch 743/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3649 - val_loss: 0.3645\n",
      "Epoch 744/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3647 - val_loss: 0.3643\n",
      "Epoch 745/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3645 - val_loss: 0.3641\n",
      "Epoch 746/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3644 - val_loss: 0.3639\n",
      "Epoch 747/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3642 - val_loss: 0.3637\n",
      "Epoch 748/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3640 - val_loss: 0.3636\n",
      "Epoch 749/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3638 - val_loss: 0.3634\n",
      "Epoch 750/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3637 - val_loss: 0.3632\n",
      "Epoch 751/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3635 - val_loss: 0.3631\n",
      "Epoch 752/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3633 - val_loss: 0.3629\n",
      "Epoch 753/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3632 - val_loss: 0.3627\n",
      "Epoch 754/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3630 - val_loss: 0.3625\n",
      "Epoch 755/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3628 - val_loss: 0.3624\n",
      "Epoch 756/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3626 - val_loss: 0.3622\n",
      "Epoch 757/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3625 - val_loss: 0.3620\n",
      "Epoch 758/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3623 - val_loss: 0.3619\n",
      "Epoch 759/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3621 - val_loss: 0.3617\n",
      "Epoch 760/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3620 - val_loss: 0.3615\n",
      "Epoch 761/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3618 - val_loss: 0.3613\n",
      "Epoch 762/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3616 - val_loss: 0.3612\n",
      "Epoch 763/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3614 - val_loss: 0.3610\n",
      "Epoch 764/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3613 - val_loss: 0.3608\n",
      "Epoch 765/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3611 - val_loss: 0.3607\n",
      "Epoch 766/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3609 - val_loss: 0.3605\n",
      "Epoch 767/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3608 - val_loss: 0.3603\n",
      "Epoch 768/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3606 - val_loss: 0.3602\n",
      "Epoch 769/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3604 - val_loss: 0.3600\n",
      "Epoch 770/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3603 - val_loss: 0.3599\n",
      "Epoch 771/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3601 - val_loss: 0.3597\n",
      "Epoch 772/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3600 - val_loss: 0.3595\n",
      "Epoch 773/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3598 - val_loss: 0.3594\n",
      "Epoch 774/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3596 - val_loss: 0.3592\n",
      "Epoch 775/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3595 - val_loss: 0.3590\n",
      "Epoch 776/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3593 - val_loss: 0.3589\n",
      "Epoch 777/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3591 - val_loss: 0.3587\n",
      "Epoch 778/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3590 - val_loss: 0.3586\n",
      "Epoch 779/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3588 - val_loss: 0.3584\n",
      "Epoch 780/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3587 - val_loss: 0.3582\n",
      "Epoch 781/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3585 - val_loss: 0.3581\n",
      "Epoch 782/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3583 - val_loss: 0.3579\n",
      "Epoch 783/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3582 - val_loss: 0.3578\n",
      "Epoch 784/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3580 - val_loss: 0.3576\n",
      "Epoch 785/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3579 - val_loss: 0.3574\n",
      "Epoch 786/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3577 - val_loss: 0.3573\n",
      "Epoch 787/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3575 - val_loss: 0.3571\n",
      "Epoch 788/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3574 - val_loss: 0.3570\n",
      "Epoch 789/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3572 - val_loss: 0.3568\n",
      "Epoch 790/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3571 - val_loss: 0.3567\n",
      "Epoch 791/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3569 - val_loss: 0.3565\n",
      "Epoch 792/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3568 - val_loss: 0.3564\n",
      "Epoch 793/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3566 - val_loss: 0.3562\n",
      "Epoch 794/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3565 - val_loss: 0.3561\n",
      "Epoch 795/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3563 - val_loss: 0.3559\n",
      "Epoch 796/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3562 - val_loss: 0.3557\n",
      "Epoch 797/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3560 - val_loss: 0.3556\n",
      "Epoch 798/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3558 - val_loss: 0.3554\n",
      "Epoch 799/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3557 - val_loss: 0.3553\n",
      "Epoch 800/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3555 - val_loss: 0.3551\n",
      "Epoch 801/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3554 - val_loss: 0.3550\n",
      "Epoch 802/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3552 - val_loss: 0.3548\n",
      "Epoch 803/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3551 - val_loss: 0.3547\n",
      "Epoch 804/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3549 - val_loss: 0.3545\n",
      "Epoch 805/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3548 - val_loss: 0.3544\n",
      "Epoch 806/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3546 - val_loss: 0.3542\n",
      "Epoch 807/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3545 - val_loss: 0.3541\n",
      "Epoch 808/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3543 - val_loss: 0.3539\n",
      "Epoch 809/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3542 - val_loss: 0.3538\n",
      "Epoch 810/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3540 - val_loss: 0.3537\n",
      "Epoch 811/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3539 - val_loss: 0.3535\n",
      "Epoch 812/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3538 - val_loss: 0.3534\n",
      "Epoch 813/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3536 - val_loss: 0.3532\n",
      "Epoch 814/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3535 - val_loss: 0.3531\n",
      "Epoch 815/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3533 - val_loss: 0.3529\n",
      "Epoch 816/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3532 - val_loss: 0.3528\n",
      "Epoch 817/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3530 - val_loss: 0.3526\n",
      "Epoch 818/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3529 - val_loss: 0.3525\n",
      "Epoch 819/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3527 - val_loss: 0.3523\n",
      "Epoch 820/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3526 - val_loss: 0.3522\n",
      "Epoch 821/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3524 - val_loss: 0.3521\n",
      "Epoch 822/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3523 - val_loss: 0.3519\n",
      "Epoch 823/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3522 - val_loss: 0.3518\n",
      "Epoch 824/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3520 - val_loss: 0.3516\n",
      "Epoch 825/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3519 - val_loss: 0.3515\n",
      "Epoch 826/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3517 - val_loss: 0.3514\n",
      "Epoch 827/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3516 - val_loss: 0.3512\n",
      "Epoch 828/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3515 - val_loss: 0.3511\n",
      "Epoch 829/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3513 - val_loss: 0.3509\n",
      "Epoch 830/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3512 - val_loss: 0.3508\n",
      "Epoch 831/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3510 - val_loss: 0.3507\n",
      "Epoch 832/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3509 - val_loss: 0.3505\n",
      "Epoch 833/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3508 - val_loss: 0.3504\n",
      "Epoch 834/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3506 - val_loss: 0.3502\n",
      "Epoch 835/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3505 - val_loss: 0.3501\n",
      "Epoch 836/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3503 - val_loss: 0.3500\n",
      "Epoch 837/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3502 - val_loss: 0.3498\n",
      "Epoch 838/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3501 - val_loss: 0.3497\n",
      "Epoch 839/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3499 - val_loss: 0.3496\n",
      "Epoch 840/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3498 - val_loss: 0.3494\n",
      "Epoch 841/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3497 - val_loss: 0.3493\n",
      "Epoch 842/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3495 - val_loss: 0.3491\n",
      "Epoch 843/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3494 - val_loss: 0.3490\n",
      "Epoch 844/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3492 - val_loss: 0.3489\n",
      "Epoch 845/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3491 - val_loss: 0.3487\n",
      "Epoch 846/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3490 - val_loss: 0.3486\n",
      "Epoch 847/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3488 - val_loss: 0.3485\n",
      "Epoch 848/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3487 - val_loss: 0.3483\n",
      "Epoch 849/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3486 - val_loss: 0.3482\n",
      "Epoch 850/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3484 - val_loss: 0.3481\n",
      "Epoch 851/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3483 - val_loss: 0.3480\n",
      "Epoch 852/1000\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3482 - val_loss: 0.3478\n",
      "Epoch 853/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3480 - val_loss: 0.3477\n",
      "Epoch 854/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3479 - val_loss: 0.3476\n",
      "Epoch 855/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3478 - val_loss: 0.3474\n",
      "Epoch 856/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3477 - val_loss: 0.3473\n",
      "Epoch 857/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3475 - val_loss: 0.3472\n",
      "Epoch 858/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3474 - val_loss: 0.3470\n",
      "Epoch 859/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3473 - val_loss: 0.3469\n",
      "Epoch 860/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3471 - val_loss: 0.3468\n",
      "Epoch 861/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3470 - val_loss: 0.3467\n",
      "Epoch 862/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3469 - val_loss: 0.3465\n",
      "Epoch 863/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3467 - val_loss: 0.3464\n",
      "Epoch 864/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3466 - val_loss: 0.3463\n",
      "Epoch 865/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3465 - val_loss: 0.3461\n",
      "Epoch 866/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3464 - val_loss: 0.3460\n",
      "Epoch 867/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3462 - val_loss: 0.3459\n",
      "Epoch 868/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3461 - val_loss: 0.3458\n",
      "Epoch 869/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3460 - val_loss: 0.3456\n",
      "Epoch 870/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3459 - val_loss: 0.3455\n",
      "Epoch 871/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3457 - val_loss: 0.3454\n",
      "Epoch 872/1000\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3456 - val_loss: 0.3453\n",
      "Epoch 873/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3455 - val_loss: 0.3451\n",
      "Epoch 874/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3454 - val_loss: 0.3450\n",
      "Epoch 875/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3452 - val_loss: 0.3449\n",
      "Epoch 876/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3451 - val_loss: 0.3448\n",
      "Epoch 877/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3450 - val_loss: 0.3446\n",
      "Epoch 878/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3449 - val_loss: 0.3445\n",
      "Epoch 879/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3447 - val_loss: 0.3444\n",
      "Epoch 880/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3446 - val_loss: 0.3443\n",
      "Epoch 881/1000\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.3445 - val_loss: 0.3441\n",
      "Epoch 882/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3444 - val_loss: 0.3440\n",
      "Epoch 883/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3442 - val_loss: 0.3439\n",
      "Epoch 884/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3441 - val_loss: 0.3438\n",
      "Epoch 885/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3440 - val_loss: 0.3437\n",
      "Epoch 886/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3439 - val_loss: 0.3435\n",
      "Epoch 887/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3438 - val_loss: 0.3434\n",
      "Epoch 888/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3436 - val_loss: 0.3433\n",
      "Epoch 889/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3435 - val_loss: 0.3432\n",
      "Epoch 890/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3434 - val_loss: 0.3431\n",
      "Epoch 891/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3433 - val_loss: 0.3429\n",
      "Epoch 892/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3432 - val_loss: 0.3428\n",
      "Epoch 893/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3430 - val_loss: 0.3427\n",
      "Epoch 894/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3429 - val_loss: 0.3426\n",
      "Epoch 895/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3428 - val_loss: 0.3425\n",
      "Epoch 896/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3427 - val_loss: 0.3424\n",
      "Epoch 897/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3426 - val_loss: 0.3422\n",
      "Epoch 898/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3425 - val_loss: 0.3421\n",
      "Epoch 899/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3423 - val_loss: 0.3420\n",
      "Epoch 900/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3422 - val_loss: 0.3419\n",
      "Epoch 901/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3421 - val_loss: 0.3418\n",
      "Epoch 902/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3420 - val_loss: 0.3417\n",
      "Epoch 903/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3419 - val_loss: 0.3415\n",
      "Epoch 904/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3418 - val_loss: 0.3414\n",
      "Epoch 905/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3416 - val_loss: 0.3413\n",
      "Epoch 906/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3415 - val_loss: 0.3412\n",
      "Epoch 907/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3414 - val_loss: 0.3411\n",
      "Epoch 908/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3413 - val_loss: 0.3410\n",
      "Epoch 909/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3412 - val_loss: 0.3409\n",
      "Epoch 910/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3411 - val_loss: 0.3407\n",
      "Epoch 911/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3410 - val_loss: 0.3406\n",
      "Epoch 912/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3408 - val_loss: 0.3405\n",
      "Epoch 913/1000\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3407 - val_loss: 0.3404\n",
      "Epoch 914/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3406 - val_loss: 0.3403\n",
      "Epoch 915/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3405 - val_loss: 0.3402\n",
      "Epoch 916/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3404 - val_loss: 0.3401\n",
      "Epoch 917/1000\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3403 - val_loss: 0.3400\n",
      "Epoch 918/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3402 - val_loss: 0.3398\n",
      "Epoch 919/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3401 - val_loss: 0.3397\n",
      "Epoch 920/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3399 - val_loss: 0.3396\n",
      "Epoch 921/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3398 - val_loss: 0.3395\n",
      "Epoch 922/1000\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3397 - val_loss: 0.3394\n",
      "Epoch 923/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3396 - val_loss: 0.3393\n",
      "Epoch 924/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3395 - val_loss: 0.3392\n",
      "Epoch 925/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3394 - val_loss: 0.3391\n",
      "Epoch 926/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3393 - val_loss: 0.3390\n",
      "Epoch 927/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3392 - val_loss: 0.3389\n",
      "Epoch 928/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3391 - val_loss: 0.3387\n",
      "Epoch 929/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3389 - val_loss: 0.3386\n",
      "Epoch 930/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3388 - val_loss: 0.3385\n",
      "Epoch 931/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3387 - val_loss: 0.3384\n",
      "Epoch 932/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3386 - val_loss: 0.3383\n",
      "Epoch 933/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3385 - val_loss: 0.3382\n",
      "Epoch 934/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3384 - val_loss: 0.3381\n",
      "Epoch 935/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3383 - val_loss: 0.3380\n",
      "Epoch 936/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3382 - val_loss: 0.3379\n",
      "Epoch 937/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3381 - val_loss: 0.3378\n",
      "Epoch 938/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3380 - val_loss: 0.3377\n",
      "Epoch 939/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3379 - val_loss: 0.3376\n",
      "Epoch 940/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3378 - val_loss: 0.3375\n",
      "Epoch 941/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3377 - val_loss: 0.3374\n",
      "Epoch 942/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3376 - val_loss: 0.3373\n",
      "Epoch 943/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3375 - val_loss: 0.3371\n",
      "Epoch 944/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3373 - val_loss: 0.3370\n",
      "Epoch 945/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3372 - val_loss: 0.3369\n",
      "Epoch 946/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3371 - val_loss: 0.3368\n",
      "Epoch 947/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3370 - val_loss: 0.3367\n",
      "Epoch 948/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3369 - val_loss: 0.3366\n",
      "Epoch 949/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3368 - val_loss: 0.3365\n",
      "Epoch 950/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3367 - val_loss: 0.3364\n",
      "Epoch 951/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3366 - val_loss: 0.3363\n",
      "Epoch 952/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3365 - val_loss: 0.3362\n",
      "Epoch 953/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3364 - val_loss: 0.3361\n",
      "Epoch 954/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3363 - val_loss: 0.3360\n",
      "Epoch 955/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3362 - val_loss: 0.3359\n",
      "Epoch 956/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3361 - val_loss: 0.3358\n",
      "Epoch 957/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3360 - val_loss: 0.3357\n",
      "Epoch 958/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3359 - val_loss: 0.3356\n",
      "Epoch 959/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3358 - val_loss: 0.3355\n",
      "Epoch 960/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3357 - val_loss: 0.3354\n",
      "Epoch 961/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3356 - val_loss: 0.3353\n",
      "Epoch 962/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3355 - val_loss: 0.3352\n",
      "Epoch 963/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3354 - val_loss: 0.3351\n",
      "Epoch 964/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3353 - val_loss: 0.3350\n",
      "Epoch 965/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3352 - val_loss: 0.3349\n",
      "Epoch 966/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3351 - val_loss: 0.3348\n",
      "Epoch 967/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3350 - val_loss: 0.3347\n",
      "Epoch 968/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3349 - val_loss: 0.3346\n",
      "Epoch 969/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3348 - val_loss: 0.3345\n",
      "Epoch 970/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3347 - val_loss: 0.3344\n",
      "Epoch 971/1000\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.3346 - val_loss: 0.3343\n",
      "Epoch 972/1000\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.3345 - val_loss: 0.3342\n",
      "Epoch 973/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3344 - val_loss: 0.3341\n",
      "Epoch 974/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3343 - val_loss: 0.3340\n",
      "Epoch 975/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3342 - val_loss: 0.3339\n",
      "Epoch 976/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3341 - val_loss: 0.3338\n",
      "Epoch 977/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3340 - val_loss: 0.3337\n",
      "Epoch 978/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3339 - val_loss: 0.3336\n",
      "Epoch 979/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3338 - val_loss: 0.3335\n",
      "Epoch 980/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3337 - val_loss: 0.3334\n",
      "Epoch 981/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3336 - val_loss: 0.3333\n",
      "Epoch 982/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3335 - val_loss: 0.3333\n",
      "Epoch 983/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3334 - val_loss: 0.3332\n",
      "Epoch 984/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3333 - val_loss: 0.3331\n",
      "Epoch 985/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3332 - val_loss: 0.3330\n",
      "Epoch 986/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3332 - val_loss: 0.3329\n",
      "Epoch 987/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3331 - val_loss: 0.3328\n",
      "Epoch 988/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3330 - val_loss: 0.3327\n",
      "Epoch 989/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3329 - val_loss: 0.3326\n",
      "Epoch 990/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3328 - val_loss: 0.3325\n",
      "Epoch 991/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3327 - val_loss: 0.3324\n",
      "Epoch 992/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3326 - val_loss: 0.3323\n",
      "Epoch 993/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3325 - val_loss: 0.3322\n",
      "Epoch 994/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3324 - val_loss: 0.3321\n",
      "Epoch 995/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3323 - val_loss: 0.3320\n",
      "Epoch 996/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3322 - val_loss: 0.3319\n",
      "Epoch 997/1000\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3321 - val_loss: 0.3318\n",
      "Epoch 998/1000\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3320 - val_loss: 0.3318\n",
      "Epoch 999/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3319 - val_loss: 0.3317\n",
      "Epoch 1000/1000\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3318 - val_loss: 0.3316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f01dc47f8d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Expected to talk about the components of autoencoder and their purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Autoencoder (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model needs to be symetrical, relu works better with images, reducing to 32\n",
    "input_img = Input(shape=(784,)) # 1st layer of the network\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded) # Fully dehydrated layer\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded) \n",
    "# Sigmoid used since input normalized between 0-1 for grayscale, for probabilistic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3385 - val_loss: 0.2489\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2216 - val_loss: 0.1909\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1749 - val_loss: 0.1586\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1516 - val_loss: 0.1434\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1412 - val_loss: 0.1363\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1350 - val_loss: 0.1308\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1296 - val_loss: 0.1256\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1256 - val_loss: 0.1225\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1226 - val_loss: 0.1200\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.1206 - val_loss: 0.1178\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1185 - val_loss: 0.1160\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.1166 - val_loss: 0.1147\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1152 - val_loss: 0.1125\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1133 - val_loss: 0.1112\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1119 - val_loss: 0.1095\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1103 - val_loss: 0.1085\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1086 - val_loss: 0.1066\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1075 - val_loss: 0.1059\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1061 - val_loss: 0.1042\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1050 - val_loss: 0.1032\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1043 - val_loss: 0.1023\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1031 - val_loss: 0.1014\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1023 - val_loss: 0.1007\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1016 - val_loss: 0.1001\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1009 - val_loss: 0.0994\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1003 - val_loss: 0.0989\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0994 - val_loss: 0.0980\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0989 - val_loss: 0.0981\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0985 - val_loss: 0.0983\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0979 - val_loss: 0.0966\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0975 - val_loss: 0.0962\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0970 - val_loss: 0.0956\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0966 - val_loss: 0.0954\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0962 - val_loss: 0.0951\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0957 - val_loss: 0.0944\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0952 - val_loss: 0.0942\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0948 - val_loss: 0.0936\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0945 - val_loss: 0.0934\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0942 - val_loss: 0.0930\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0938 - val_loss: 0.0927\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0934 - val_loss: 0.0922\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0926 - val_loss: 0.0916\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0922 - val_loss: 0.0915\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0919 - val_loss: 0.0909\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0916 - val_loss: 0.0905\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0914 - val_loss: 0.0910\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0910 - val_loss: 0.0900\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0908 - val_loss: 0.0900\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0906 - val_loss: 0.0895\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0903 - val_loss: 0.0894\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0901 - val_loss: 0.0892\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0899 - val_loss: 0.0888\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0897 - val_loss: 0.0888\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0895 - val_loss: 0.0887\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0895 - val_loss: 0.0884\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0891 - val_loss: 0.0882\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0890 - val_loss: 0.0881\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0888 - val_loss: 0.0882\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0887 - val_loss: 0.0880\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0885 - val_loss: 0.0882\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0884 - val_loss: 0.0878\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0883 - val_loss: 0.0876\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0882 - val_loss: 0.0874\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0881 - val_loss: 0.0876\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0879 - val_loss: 0.0871\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0878 - val_loss: 0.0871\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0877 - val_loss: 0.0869\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0877 - val_loss: 0.0871\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0875 - val_loss: 0.0867\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0874 - val_loss: 0.0867\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0872 - val_loss: 0.0866\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0872 - val_loss: 0.0865\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0871 - val_loss: 0.0864\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0870 - val_loss: 0.0862\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0868 - val_loss: 0.0860\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0868 - val_loss: 0.0861\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0867 - val_loss: 0.0859\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0866 - val_loss: 0.0861\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0866 - val_loss: 0.0860\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0864 - val_loss: 0.0859\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0864 - val_loss: 0.0857\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0862 - val_loss: 0.0856\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0861 - val_loss: 0.0855\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0861 - val_loss: 0.0856\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0860 - val_loss: 0.0854\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0859 - val_loss: 0.0853\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0858 - val_loss: 0.0853\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0858 - val_loss: 0.0852\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0857 - val_loss: 0.0852\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0856 - val_loss: 0.0852\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0855 - val_loss: 0.0850\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0855 - val_loss: 0.0848\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0853 - val_loss: 0.0848\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0854 - val_loss: 0.0848\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0852 - val_loss: 0.0847\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0852 - val_loss: 0.0846\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0851 - val_loss: 0.0844\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0850 - val_loss: 0.0844\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0850 - val_loss: 0.0844\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0850 - val_loss: 0.0843\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0849 - val_loss: 0.0846\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0849 - val_loss: 0.0843\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0848 - val_loss: 0.0843\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0848 - val_loss: 0.0842\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0846 - val_loss: 0.0843\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0846 - val_loss: 0.0842\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0846 - val_loss: 0.0841\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0846 - val_loss: 0.0841\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0845 - val_loss: 0.0840\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0844 - val_loss: 0.0839\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0844 - val_loss: 0.0841\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0843 - val_loss: 0.0839\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0843 - val_loss: 0.0840\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0843 - val_loss: 0.0839\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0842 - val_loss: 0.0839\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0842 - val_loss: 0.0837\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0841 - val_loss: 0.0837\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0840 - val_loss: 0.0836\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0840 - val_loss: 0.0837\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0840 - val_loss: 0.0835\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0840 - val_loss: 0.0839\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0840 - val_loss: 0.0834\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0839 - val_loss: 0.0838\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0839 - val_loss: 0.0835\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0838 - val_loss: 0.0833\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0838 - val_loss: 0.0835\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0837 - val_loss: 0.0834\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0838 - val_loss: 0.0834\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0837 - val_loss: 0.0834\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0837 - val_loss: 0.0833\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0836 - val_loss: 0.0833\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0836 - val_loss: 0.0831\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0836 - val_loss: 0.0831\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0835 - val_loss: 0.0832\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0835 - val_loss: 0.0833\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0835 - val_loss: 0.0830\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0834 - val_loss: 0.0830\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0834 - val_loss: 0.0829\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0833 - val_loss: 0.0829\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0833 - val_loss: 0.0832\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0833 - val_loss: 0.0830\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0833 - val_loss: 0.0830\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0833 - val_loss: 0.0829\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0832 - val_loss: 0.0829\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0832 - val_loss: 0.0828\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0831 - val_loss: 0.0827\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0831 - val_loss: 0.0828\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0831 - val_loss: 0.0828\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0830 - val_loss: 0.0827\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0830 - val_loss: 0.0827\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0829 - val_loss: 0.0828\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0830 - val_loss: 0.0826\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0829 - val_loss: 0.0828\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0829 - val_loss: 0.0825\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0828 - val_loss: 0.0824\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0828 - val_loss: 0.0825\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0828 - val_loss: 0.0824\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0827 - val_loss: 0.0824\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0827 - val_loss: 0.0824\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0827 - val_loss: 0.0824\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0827 - val_loss: 0.0824\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0827 - val_loss: 0.0823\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0826 - val_loss: 0.0823\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0825 - val_loss: 0.0823\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0826 - val_loss: 0.0822\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0825 - val_loss: 0.0822\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0825 - val_loss: 0.0822\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0825 - val_loss: 0.0824\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0825 - val_loss: 0.0820\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0824 - val_loss: 0.0823\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0824 - val_loss: 0.0821\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0824 - val_loss: 0.0821\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0824 - val_loss: 0.0819\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0823 - val_loss: 0.0820\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0823 - val_loss: 0.0821\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0822 - val_loss: 0.0821\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0823 - val_loss: 0.0821\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0822 - val_loss: 0.0820\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0822 - val_loss: 0.0821\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0822 - val_loss: 0.0818\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0822 - val_loss: 0.0819\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0821 - val_loss: 0.0820\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0821 - val_loss: 0.0819\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0821 - val_loss: 0.0819\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0821 - val_loss: 0.0818\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0820 - val_loss: 0.0818\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0820 - val_loss: 0.0821\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0820 - val_loss: 0.0819\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0820 - val_loss: 0.0818\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0820 - val_loss: 0.0819\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0820 - val_loss: 0.0818\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0819 - val_loss: 0.0818\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0819 - val_loss: 0.0818\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0819 - val_loss: 0.0818\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0819 - val_loss: 0.0816\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0819 - val_loss: 0.0816\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0819 - val_loss: 0.0816\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0818 - val_loss: 0.0818\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0819 - val_loss: 0.0816\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0818 - val_loss: 0.0816\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0818 - val_loss: 0.0817\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0818 - val_loss: 0.0816\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0818 - val_loss: 0.0817\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0818 - val_loss: 0.0815\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0817 - val_loss: 0.0815\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0817 - val_loss: 0.0816\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0817 - val_loss: 0.0815\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0817 - val_loss: 0.0815\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0817 - val_loss: 0.0816\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0816 - val_loss: 0.0816\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0817 - val_loss: 0.0816\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0816 - val_loss: 0.0814\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0816 - val_loss: 0.0814\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0816 - val_loss: 0.0815\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0816 - val_loss: 0.0814\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0816 - val_loss: 0.0814\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0816 - val_loss: 0.0815\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0816 - val_loss: 0.0814\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0816 - val_loss: 0.0814\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0816 - val_loss: 0.0815\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0814\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0817\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0815 - val_loss: 0.0814\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0814\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0813\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0813\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0812\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0815\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0812\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0815 - val_loss: 0.0814\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0813\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0813\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0812\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0812\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0813\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0814 - val_loss: 0.0812\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0812\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0814 - val_loss: 0.0812\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0813\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0812\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0813\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0814 - val_loss: 0.0813\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0813 - val_loss: 0.0812\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0813\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0811\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0813\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0813 - val_loss: 0.0813\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0811\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0813\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0811\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0811\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0813\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0812 - val_loss: 0.0811\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0812 - val_loss: 0.0810\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0812\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0813\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0812 - val_loss: 0.0812\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0810\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0813\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0814\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0811 - val_loss: 0.0810\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0810\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0811 - val_loss: 0.0810\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0810 - val_loss: 0.0808\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0811 - val_loss: 0.0810\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0811 - val_loss: 0.0812\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0809\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0810\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0811\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0810\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0808\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0809\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0810 - val_loss: 0.0809\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0810 - val_loss: 0.0810\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0810 - val_loss: 0.0808\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0809\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0810 - val_loss: 0.0809\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0810 - val_loss: 0.0809\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0809 - val_loss: 0.0810\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0809 - val_loss: 0.0810\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0809 - val_loss: 0.0808\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0809 - val_loss: 0.0809\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0807\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0810 - val_loss: 0.0812\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0809\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0809\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0809\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0809 - val_loss: 0.0811\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0809 - val_loss: 0.0808\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0809 - val_loss: 0.0809\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0808\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0811\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0807\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0808\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0809 - val_loss: 0.0808\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0810\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0808 - val_loss: 0.0808\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0808 - val_loss: 0.0807\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0808\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0807 - val_loss: 0.0808\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0809\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0806\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0807\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0808\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0806\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0807\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0809\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0807 - val_loss: 0.0807\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0806\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0805\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0808\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0807\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0808\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0808\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0806 - val_loss: 0.0806\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0806\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0806\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0805\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0805\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0805\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0808\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0806 - val_loss: 0.0806\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0806 - val_loss: 0.0805\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0804\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0807\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0805 - val_loss: 0.0804\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0805 - val_loss: 0.0804\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0805 - val_loss: 0.0806\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0805 - val_loss: 0.0808\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0805 - val_loss: 0.0805\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0805\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0803 - val_loss: 0.0805\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0804 - val_loss: 0.0804\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0803 - val_loss: 0.0805\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0805\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0805\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0804\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0804\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0803\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0803 - val_loss: 0.0802\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0802 - val_loss: 0.0804\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0804\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0801 - val_loss: 0.0803\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0802 - val_loss: 0.0804\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0801 - val_loss: 0.0803\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0803\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0801 - val_loss: 0.0803\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0804\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0802 - val_loss: 0.0802\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0801\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0802 - val_loss: 0.0803\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0803\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0804\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0803\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0801\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0801\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0801 - val_loss: 0.0801\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0803\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0805\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0801 - val_loss: 0.0802\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0803\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0800\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0800\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0799 - val_loss: 0.0800\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0803\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0800\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0799 - val_loss: 0.0802\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0801\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0803\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0800 - val_loss: 0.0800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f01acc11f60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile & fit model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=500,\n",
    "                batch_size=784,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolutional autoencoder\n",
    "\n",
    "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# Create Model - Convolutions with UpSampling\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(16, (3,3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2,2), padding='same')(x)\n",
    "\n",
    "# At this point the representation is (4, 4, 8) i.e. 128-dimensional representation\n",
    "\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(16, (3,3), activation='relu')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# No dense layers, only convolutions\n",
    "autoencoder.summary()\n",
    "\n",
    "# 16 - 28x28 feature maps (2nd layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"mnist_autoencoder\", entity=\"ds5\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=784,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoder.predict(x_train)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will train an autoencoder at some point in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with Autoencoders (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
    "\n",
    "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39389127,  1.0158104 ,  0.        ,  0.        ,  0.06517133,\n",
       "        2.450819  ,  0.        ,  5.1117034 ,  0.74338543,  2.3620906 ,\n",
       "        0.        ,  0.        ,  0.        ,  2.0215404 ,  6.1629906 ,\n",
       "        0.6670714 ,  4.66508   ,  2.5439487 , 17.914988  ,  0.        ,\n",
       "        7.9524546 ,  5.3824563 ,  1.0916216 ,  6.234546  ,  0.        ,\n",
       "        0.8884269 ,  7.485719  ,  3.44194   ,  8.927442  ,  0.        ,\n",
       "        0.23894644,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs[0].reshape((128,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nn.fit(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "    - Enocder\n",
    "    - Decoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "    - Can do in Keras Easily\n",
    "    - Can use a variety of architectures\n",
    "    - Architectures must follow hourglass shape\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "    - Extract just the encoder to use for various tasks\n",
    "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "__References__\n",
    "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
    "\n",
    "__Additional Material__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
